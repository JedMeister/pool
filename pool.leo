<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet ekr_test?>
<leo_file>
<leo_header file_format="2" tnodes="0" max_tnode_index="2" clone_windows="0"/>
<globals body_outline_ratio="0.285115303983">
	<global_window_position top="0" left="0" height="954" width="820"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="loni.20100827184947" a="E"><vh>Project</vh>
<v t="loni.20100827184947.1" a="E"><vh>design</vh>
<v t="loni.20100827184947.2"><vh>brainstorming</vh>
<v t="zaril.20100828060043"><vh>first draft</vh></v>
</v>
<v t="zaril.20100828055849"><vh>terminology</vh></v>
<v t="loni.20100827184947.4"><vh>file/data structure</vh></v>
<v t="loni.20100827184947.5"><vh>cli syntax</vh></v>
<v t="loni.20100827184947.6"><vh>usage examples</vh></v>
</v>
<v t="loni.20100827184947.7" a="E"><vh>implementation</vh>
<v t="zaril.20101018013654"><vh>v0.9</vh>
<v t="zaril.20100906050211" a="E"><vh>implement full command set for binaries in regular directories</vh></v>
<v t="zaril.20100906050211.1"><vh>init /path/to/build-chroot</vh></v>
<v t="zaril.20100906050211.2"><vh>register / unregister # regular directories</vh></v>
<v t="zaril.20100906230204"><vh>info</vh></v>
<v t="zaril.20100906050211.4"><vh>exists</vh>
<v t="zaril.20100907030232"><vh>refactor away stock</vh></v>
</v>
<v t="zaril.20100906050211.5"><vh>list</vh></v>
<v t="zaril.20100906050211.6"><vh>get</vh></v>
<v t="zaril.20100909033533"><vh>pool contains pool</vh></v>
<v t="zaril.20100910205006"><vh>BUG: sync not importing binaries?</vh></v>
<v t="zaril.20100909050527"><vh>support regular sources in regular directories</vh>
<v t="zaril.20100911052023" a="M"><vh>setup a local caching proxy</vh>
<v t="zaril.20100912044439" a="M"><vh>fix squid</vh></v>
<v t="zaril.20100912052909"><vh>configure jaunty to use apt-proxy</vh></v>
<v t="zaril.20100912065617"><vh>apt-proxy README</vh></v>
<v t="zaril.20100912052439"><vh>test debootstrap with caching proxy</vh></v>
</v>
<v t="zaril.20100910021430" a="M"><vh>research existing solutions</vh>
<v t="zaril.20100910040422.2" a="M"><vh>explore cowdancer</vh></v>
<v t="zaril.20100911012707" a="M"><vh>explore fl-cow</vh></v>
<v t="zaril.20100912055420" a="M"><vh>exlore apt-move</vh></v>
<v t="zaril.20100911004946" a="E"><vh>explore pbuilder</vh>
<v t="zaril.20100918092126"><vh>alternatives</vh></v>
</v>
<v t="zaril.20100923230802" a="M"><vh>explore sbuild</vh></v>
<v t="zaril.20100916032546" a="M"><vh>explore build dependencies</vh></v>
<v t="zaril.20100911091317"><vh>research suid</vh></v>
</v>
<v t="zaril.20100923043734" a="EM"><vh>exists</vh></v>
<v t="zaril.20100923071058"><vh>list</vh></v>
<v t="zaril.20100923072629"><vh>get</vh></v>
</v>
<v t="zaril.20100930001758"><vh>sort debian packages according to dpkg algorithm</vh></v>
<v t="zaril.20100930010726"><vh>fix getpath to get newest packages?</vh>
<v t="zaril.20100930021923"><vh>pool.Pool().list() can not be invoked twice</vh></v>
</v>
<v t="zaril.20100923143609"><vh>support complex stock types</vh>
<v t="zaril.20100929193931" a="E"><vh>test/develop lightweight branch cloning</vh></v>
<v t="zaril.20101003042126"><vh>support Git branches</vh>
<v t="zaril.20101003051253" a="M"><vh>register git branches</vh></v>
<v t="zaril.20101003051313" a="M"><vh>fix info</vh></v>
<v t="zaril.20101003051253.1" a="EM"><vh>unregister git branches</vh></v>
<v t="zaril.20101003054139" a="M"><vh>exists</vh>
<v t="zaril.20101003091746"><vh>Stock.sync</vh>
<v t="zaril.20101005045129" a="E"><vh>light update of light clone</vh></v>
<v t="zaril.20101005065057"><vh>copy all tags</vh></v>
</v>
</v>
<v t="zaril.20101003054139.1" a="M"><vh>list</vh></v>
<v t="zaril.20101003054139.2"><vh>get</vh></v>
</v>
<v t="zaril.20101006063626"><vh>fix subpools</vh></v>
<v t="zaril.20101006073117" a="M"><vh>optimize Stock attributes</vh></v>
<v t="zaril.20101005094458"><vh>support Sumo branches</vh></v>
</v>
<v t="zaril.20101005221943" a="EM"><vh>extend pool-info</vh></v>
<v t="zaril.20101007060755"><vh>pool-info-build package</vh>
<v t="zaril.20101007101419"><vh>add build logs to pool-info</vh></v>
<v t="zaril.20101007100036"><vh>support binary names</vh></v>
</v>
<v t="zaril.20101006120036"><vh>fix pool-info handling of --stocks and --subpools?</vh></v>
<v t="zaril.20101011042720"><vh>bug: debversion.compare mis-calculating integers</vh>
<v t="zaril.20101011052050"><vh>optimize</vh></v>
<v t="zaril.20101011074222"><vh>normalize bug</vh></v>
</v>
</v>
<v t="zaril.20101018013654.1" a="E"><vh>unreleased</vh>
<v t="zaril.20101011023841" a="EM"><vh>bug: deb_cmp_versions doesn't work correctly</vh></v>
<v t="zaril.20101014093949" a="M"><vh>bug: lexical comparison in debversion is wrong</vh></v>
<v t="zaril.20101018013654.3" a="EM"><vh>pool-gc</vh></v>
<v t="zaril.20101018204912" a="M"><vh>remove cached packages when we unregister a stock</vh></v>
<v t="zaril.20101018013654.2" a="M"><vh>optimize pool-get'ing a cached binary</vh>
<v t="zaril.20101019010153"><vh>add internal cache to PackageCache</vh></v>
<v t="zaril.20101019010223"><vh>make syncing optional</vh></v>
</v>
<v t="zaril.20101021175833" a="EM"><vh>bug: pool-info-build broken</vh></v>
<v t="zaril.20101025111704" a="M"><vh>bug: pool doesn't reproduce thin branch</vh></v>
<v t="zaril.20101105195018" a="TV"><vh>setup regression testing suite</vh>
<v t="zaril.20101112071603"><vh>setup pyhello</vh></v>
</v>
<v t="zaril.20101111222506"><vh>clean up pool code?</vh></v>
</v>
</v>
<v t="zaril.20101007151726" a="E"><vh>bugs/wishlist</vh>
<v t="zaril.20101101154405"><vh>bug: permission problems when root runs the pool</vh></v>
<v t="zaril.20101022061317"><vh>bug: pool assumes that packages have always existed in all versions</vh></v>
<v t="zaril.20101009193905"><vh>accept multi-arguments in register/unregister?</vh></v>
<v t="zaril.20101007101547"><vh>auto less output in pool-info? / pool-info-build</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="loni.20100827184947">@nocolor
</t>
<t tx="loni.20100827184947.1">The pool is a system thats maintains a pool of binary packages which may be either imported as-is from a registered collection of binaries, or built on-demand from a registered source or collection of sources.

The dominant abstraction for the pool is a virtual filesystem 
folder that you can get get specific versions of specific packages from. Behind the scenes, the pool will build those binary packages for from source if required and cache the built binaries for future access.

In this sense calling it a pool is in line with software terminology conventions [http://en.wikipedia.org/wiki/Object_pool].</t>
<t tx="loni.20100827184947.2">* IDEAS
change untagged auto-versioning to always be smaller than tagged autoversioning

auto-versioning a separate program?
    this way we could re-use the code for pyproject's auto-versioning

leave failed builds in temporary directory
    so we can diagnose problem manually

    $TMPDIR/pool/&lt;poolname&gt;/&lt;package&gt;

figure out how to allow packages to be built manually
    unversioned - for testing?

psuedo-logic for getting a single package=version
    update binary cache 
        scan all stock deltas for new binary packages to put into our cache
    if lookup package in binary cache:
        return package
    
    update source versions
        scan all stock deltas for new debian source packages
            a debian package cannot be inside another debian package
        scan control file to get package name list and generate new auto-versions
        output new auto-versions into pool's versions cache

    lookup stock, relative_path for requested package=version in version cache
    recalculate commit-id for stock, check that version is legal
    seek to commit, extract source for debian package
    build source for debian package in build environment
    insert built packages into binary cache

* QUESTIONS
Q: what should we call the system?
A:
    binpool
        pool of binaries
        separate `srcpool' system will need to be developed later to comply with licensing restrictions

---
Q: do we need a binary cache for the versions information? (e.g., SQLite)
A:

probably not - don't worry about performance until after you have benchmarks 
    not sure it matters
    premature optimization is the root of all evil

ideal solution - figure out how to store the information directly in the filesystem
    format should be simple and easy to access

---
INSIGHT: debian package can not contain another debian package so scanning containers is a shallow, quick
operation
---
Q: how does auto-versioning for Sumo arena work?
GOTCHA: sumo auto-versioning is tricky
	is the number of commits the same on different fat branches?
	
	GOTCHA: fat thin commits don't actually include directory modifications

    IDEAS:
        maybe we only count commits on thin branch?
        maybe only count overlays as revisions?
            logic would then be very similar to Git repo logic
    
            what happens if we apply a patch to the fat?
                shouldn't that auto-increment?
        manual incrementation of auto-version?
    
---
Q: how does auto-versioning work for multi-debian packagse in a regular Git repository?
---
Q: do we support different versioning for single-debian package and multi-debian packages?
---
Q: can we support object caching safely?
A:
    if its content addressable then yes
</t>
<t tx="loni.20100827184947.4">Notes: 
* The pools tucks away all of its internals neatly out of site in a `hidden' directory. The reason for doing this was to make it easy to treat the pool as a `special' directory, which may contain registered stocks or subpools as sub-directories.

* The pool has no data formats/configuration files. All of its `data' is stored directly as filesystem constructs.

.pool/
    build/
        root -&gt; /path/to/buildroot # symbolic link
        logs/
            &lt;package&gt;-&lt;version&gt;.build
                log of the build process
       
    pkgcache/
        &lt;package&gt;-&lt;version&gt;.&lt;arch&gt;.deb # maybe in a pool-like tree
    
    stock/
        &lt;name&gt;#&lt;branch&gt;/
            link -&gt; /path/to/stock # symbolic link to the stock
    
            source-versions/
                &lt;relative-path&gt;/&lt;package&gt;
                    contains cached list of versions

            SYNC_HEAD # contains the last checkout/.git/HEAD we synced against
            checkout/
            


    
    



    </t>
<t tx="loni.20100827184947.5">=== ENVIRONMENT

    POOL_DIR        defaults to cwd

=== CLI SYNTAX

* init /path/to/build-chroot
    initialize a new pool

* register /path/to/stock
    register a package stock with the pool
    stock type can be:
        another pool
            (need to watch out for circular dependencies)
        /path/to/sumo_arena[#branch]
        /path/to/git_repository[#branch]
        /path/to/regular_directory

* unregister stock
    unregister a stock from the pool

* info
    show pool information (registered stock, etc.)
    --registered            show stocks and subpools (defaults)
    --stocks                show stocks
    --subpools              show sub-pools

    --buildroot             show buildroot
    --pkgcache              show package cache
    --source-versions       show source versions

    -r --recursive        Lookup pool info recursively in subpools

* info-build package 
    will return info on built package
        or no information if it wasn't built
        or an error that the package doesn't exist

* get [-options] &lt;output-dir&gt; [ package[=version] ... ]
Get packages from pool

  If a package is specified without a version, get the newest package.
  If no packages are specified as arguments, get all the newest packages.

Options:
  -i --input &lt;file&gt;     file from which we read package list (- for stdin)

  -s --strict           fatal error on missing packages
  -q --quiet            suppress warnings about missing packages

  -t --tree             output dir is in a package tree format (like a repository)
  
* list [ &lt;package-glob&gt; ]
    if &lt;package-glob&gt; is provided, print only those packages whose names match the glob
    otherwise, by default, print a list of the newest packages

    -a --all-versions
        print all available versions of a package in the pool

    -n --name-only
        print only the names of packages in the pool (without the list)
            incompatible with -a option

* exists package[=version]
Check if package exists in pool

Prints true/false if &lt;package&gt; exists in the pool.
If true exitcode = 0, else exitcode = 1

=== FUTURE CLI?

register-block /path/to/stock package[=version]
    register a block on a specific package inside a specific stock
        this blocks the package from being included in the pool

unregister-block /path/to/stock package[=version]
    unregister the block

open
    open the FUSE-based filesystem representation of the pool

close
    close the FUSE-based filesystem representation of the pool</t>
<t tx="loni.20100827184947.6">cd pools

mkdir private
cd private

# initialize a new pool
pool-init /chroots/rocky-build

for p in /turnkey/projects/*; do
    # auto identifies the type of the stock we register
    pool-register $p
done
    
pool-info
    show pool information (registered containers, etc.)

#  woops, noticed I registered the wrong branch
#  added #devel branch for emphasis 
#  unregister would work without it since there is only one branch registered for that path
pool-unregister /turnkey/projects/covin#devel

# prints a list of all packages in the pool (by name only)
pool-list -n

# prints a list of all packages + newest versions
pool-list
  
# prints a list of all packagse that match this glob
pool-list turnkey-*

# prints a list of all package versions for neverland
pool-list --all neverland

# prints a loooong list of all package versions, old and new, for all packages
# watch out, every git commit in an autoversioned project is a new virtual version
pool-list --all

for name in $(pool-list -n); do
    if ! exists -q $name; then
        echo insane: package $name was just here a second ago
    fi
done

mkdir /tmp/newest

# gets all the newest packages in the pool to /tmp/newest
pool-get /tmp/newest 

# gets the newest neverland to /tmp/newest
pool-get /tmp/newest neverland

# gets neverland 1.2.3 specifically to /tmp/newest
pool-get /tmp/newest neverland=1.2.3

# gets all packages that are listed in product-manifest and exist in our pool to /tmp/newest
# don't warn us about packages which don't exist (unsafe)
pool-get /tmp/newest -q -i /path/to/product-manifest

# creates a repository like 
mkdir /tmp/product-repo
for package in $(cat /path/to/versioned-product-manifest); do
    if pool-exists -q $package; then
        pool-get /tmp/product-repo --tree -s $package
    fi
done</t>
<t tx="loni.20100827184947.7">ROADMAP
support Sumo arenas
</t>
<t tx="zaril.20100828055849">pool
    a system for which maintains a pool of binary packages built on-demand from registered stock

stock
    a container for packages in binary or source form

stock types
    git single
        git repository containing one single package

    git
        git repository containing multiple packages

    plain
        regular directory containing multiple packages

    sumo
        sumo arena containing multiple packages

    subpool
        another pool

binary
    a binary Debian package inside a stock

source
    a source Debian package inside a stock

build root
    a chroot environment with an suitable toolchain setup for building packages

build log
    the output from the build process for a package built from source

package cache
    a cache of binary packages either built from source or imported as-is
</t>
<t tx="zaril.20100828060043">Q: how do i name this project?

alternatives
    pms (package management system), and you know what else...
    tklps (TurnKey Linux Packaging System)
    spkg (turnkey package ...) - used by slackware
    repoware (repository software)
    poolman (pool management software)
    
A: poolman
    it is the software that manages our pool of packages, see objectives

----

Q: should we have seperate reps per release?
A:  easier to manage
    different toolchain for different releases, poolman also handles the building of packages
    different releases generally mean major updates
    this can be transparent to fab
    smaller global pool, not inifinate in size
    we can retire / archive older release pools
    A: yes

----

Q: what do i call the action/outcome of passing a manifest to poolman?

IDEA: time travel
      a release has a life span, and we can travel forward and backwards on the timeline

action
    travel
    visit
    move
    pluck
    checkout
    peak
    seek
    commit
    make
    generate*
    branch
    
outcome
    instance*
    state
    period
    entity
    frame
    snapshot

latest?
    latest
    present
    head
    current


A: the action is to generate, the outcome is an instance.</t>
<t tx="zaril.20100906050211">
</t>
<t tx="zaril.20100906050211.1">initialize a new pool

* init /path/to/build-chroot
    initialize a new pool

* logic
    mkdir .pool/{build,binaries,sources}
    mkdir .pool/build/logs
    link .pool/build to buildroot

</t>
<t tx="zaril.20100906050211.2">* register /path/to/stock
    register a package stock with the pool
    stock type can be:
        another pool
            (need to watch out for circular dependencies)
        /path/to/sumo_arena[#branch]
        /path/to/git_repository[#branch]
        /path/to/regular_directory


rescanned every time
    we can't tell if they've changed
no auto-versioning

* register logic
    raise exception if stock is already registered
    if it isn't, register stock by
        creating stock/&lt;stock_name&gt;
        creating symbolic link
            ln -s &lt;dir&gt; stock/&lt;stock_name&gt;/path

* unregister logic
    raise "stock not registered" exception if  dir doesn't match stock/path
    rmdir .pool/stock/name
    </t>
<t tx="zaril.20100906050211.4">TESTS: 
    register a directory with one binary package
    register another directory with two more packages

cli syntax
    exists package[=version]
        prints true/false if the package exists in the pool
        if true exitcode = 0, false = 1
    
logic
    sync pool with the registered stocks
    check if any binaries exist

sync logic
    search all registered stocks recursively for binary packages
    if we find a new package (that we don't already have)
        hardlink/copy into cache
    

    </t>
<t tx="zaril.20100906050211.5">* list [ &lt;package-glob&gt; ]
    if &lt;package-glob&gt; is provided, print only those packages whose names match the glob
    otherwise, by default, print a list of the newest packages

    -a --all-versions
        print all available versions of a package in the pool

    -n --name-only
        print only the names of packages in the pool (without the version)
            incompatible with -a option</t>
<t tx="zaril.20100906050211.6">logic
    if no arguments, list the newest packages

    check if package exists
        raise error if it doesn't and we're strict
        warn if we're not strict unless we're quiet

    for each package:
        if tree mode:
            calc treedir (outputdir+...)
            mkdir -p treedir

        ask the pool to output that package to the outputdir


    </t>
<t tx="zaril.20100906230204">* logic
    list .pool/stock subdirectories
    for each stock:
        print its path
</t>
<t tx="zaril.20100907030232">class PoolStocks:
    def __init__(self):
        pass

</t>
<t tx="zaril.20100909033533">ROADMAP
    solve typing issue
    proxy all pool functions
    handle circular dependencies

Q: how do we `identify' pools?
A:
    try to initialize pool object at link path?
        otherwise its a regular directory
---
Q: how do I prevent cyclic subpools?
A:
    track recursed_paths. Prevent 
</t>
<t tx="zaril.20100909050527">* ROADMAP
exists
list
get

* TEST:
    add debian package (buildable)
    link to real buildroot

* IDEAS
get_sources
    path/to/dir/package-name: &lt;version&gt;

add all legally listed binary packages files we build to cache
    (so we don't have to rebuild the same package multiple times)

extract package to package build directory
    
we need to install build dependencies to the buildroot  
    with apt-get
   </t>
<t tx="zaril.20100910021430">
* TODO
explore building packages with pbuilder
    Q: does pbuilder need root privileges?
    Q: can we use fakeroot?
    Q: how does pbuilder work?

</t>
<t tx="zaril.20100910040422.2">SUMMARY

how it works
    LD_PRELOAD trick
    directory needs to be scanned by cow-shell command
    hardlinks are always broken
    supports a pbuilder backend

usage
    cp -al src dst
    cd dst &amp;&amp; cow-shell
        file hardlinked - wil be copied to a new file before write

usage with chroot
    cow-shell invoked inside chroot.
        e.g. chroot /path/to/chroot cow-shell command-to-execute


* RESOURCE: http://blog.wiz.ro/articles/Building%20Debian%20packages%20with%20cowbuilder.pod
    explains how to use cowbuilder variant of pbuilder

cowdancer is copy-on-write upon directory trees copied with cp -la

pdebuild --pbuilder cowbuilder 


    </t>
<t tx="zaril.20100910205006"></t>
<t tx="zaril.20100911004946">Q: does the pbuilder environment setup mounts?

TODO
    read manual
    experiment with pbuilder as is
    hack it to support deck

SUMMARY

an option for saving the chroot environment instead of deleting it afterwards

pbuilder is a file year old program

its possible to run pdebuild with sudo
    
but creating the pbuilder environment requires root privileges

how does it work?
    unpacks a tarball
    install build dependencies
    gains privileges with fakeroot
        doesn't have to be real root
    builds package
    

pbuilder needs root privileges to work

uses cdebootstrap instead of debootstrap (bydefault)

by default, only supports one build environment at a time
    you can specify --basetgz option though

build dependencies need to be installed non-interactively
    some packages don't install non-interactively

debian sweeps package with pbuilder

some builds don't finish in a finite time?

sbuild - perl script
    used in buildd system
    parses Build-Depends
    perform various checks
        includes many hacks to get thsi to build
        table of which packages to substitute for virtual packages


IDEAS
    reimplement pbuilder concepts with deck

	
INSIGHT: cowdancer is not secure for isolation purposes
    hardlinks could be modified


RESOURE: http://apsy.gse.uni-magdeburg.de/main/index.psp?page=hanke/pbuilder-ssh&amp;lang=en

pbuilder-ssh: remote front-end for pbuilder
    not part of the distribution

RESOURCE: pbuilder User manual (in /usr/share/doc/pbuilder)

pbuilder doesn't try to guess what the package needs

    if a choice is to be made - tries to make the worst choice

pbuilder create
    create base chroot image with debootstrap
    --distribution 
</t>
<t tx="zaril.20100911012707">same principle as cowdancer, slightly different implementation

fl-cow will apply to any directory inside FLCOW_PATH

</t>
<t tx="zaril.20100911052023">* SUMMARY
what is this good for?
    we don't want to retrieve build dependencies over the network over and over again
        IDEA: patch apt-proxy so it doesn't hang if it can't retrieve a new Packages/Sources
        
    debootstrap of build roots

two types of proxies
    general (squid)
    application specific (apt-proxy, apt-cacher, approx)

which to choose
    apt-proxy
        pros
            standard, featureful, mature
            known to work with most programs
            can be easily changed
            works inside chroot
            supports decoupling of backend/frontend (also true for approx)
                with multiple redundant backends
            supports remote python console for debugging

            comes preconfigured with backends for debian, security , ubuntu and ubuntu-security

        cons
            bloat?

privileges required
    apt-proxy runs as aptproxy
    approx runs as user approx
    apt-cacher runs as www-data
    approx, wants root?

squid
    useful for non-Debian uses (e.g., Sumo)
    configure in apt.conf via http::proxy variable
        http://[[user][:pass]@]host[:port]/

    gotcha - you don't want to ignore no-caching options
        we need it for some files (e.g., archive files - Packages)

    cons - complex configuration

apt-proxy       Debian archive proxy and partial mirror builder
    now in version 2 (looks redesigned)
        v1 was written in shell script
        rewritten in python by 'rany' (Manuel Estrada Sainz)
            died in tragic car accident

    most complex program

    known to work with
        debootstrap
        rootstrap
        pbuilder

    written in python
        4000 LOC
        uses twisted web

    features
        importing of apt cache into archive using apt-proxy-import
        rsync for rsyncd backend
        cache cleaning of ununsed/old versions
        archive kept up to date via: ftp, http, rsync

    usage
        you configure apt to point to apt-proxy instead of the real archive
    

        configure global [DEFAULT] section
            address
            port
            min_refresh_delay       # how often to refresh Packages and other control files (default off)
            max_versions            # how many versions of a package to keep at maximum (off by default)
                
        for each resource (e.g., debian, backports) configure

            backends
                multiple backends supported for failover

    gotcha
        apt-proxy-import broken on Ubuntu
        does it import from apt-move repository?
            no it doesn't

    received bad reviews
    runs as user aptproxy
    
apt-cacher      caching proxy system for Debian package and source files
    drop-in replacement for apt-proxy
    perl program
        1700 LOC

    support utilities
        remove obsolete package files
        usage reports
        experimental features

    usage
        append proxy address to sources
            deb http://mirror.aarnet.edu.au/debian unstable main contrib non-free
                becomes

            deb http://proxy.example.com:3142/mirror.aarnet.edu.au/debian unstable main contrib non-free

        only supports http
            doesn't support ftp, rsync

        can be configured to run 
            standalone (recommended)
            as an inetd server
            CGI script (not recommended)

        interesting configuration options
            offline_mode        avoid outgoing connections completely
            expiry_hours        how many hours Package/Release files cached before assumed too old

        you can import debs
            putting them into import/ subdir of cache
            run /usr/share/apt-cacher/apt-cacher-import.pl

    pros
        offline mode

    cons
        doesn't decouple front-end APT configuration from backend configuration
        written in perl

    gotcha
        daemon_addr=localhost # doesn't work!
            daemon_addr=127.0.0.1
        
        edit /etc/default/apt-cacher
            AUTOSTART=1
            
                otherwise /etc/init.d/apt-cacher doesn't start

       
approx      caching proxy server for Debian archive files
    replacement for apt-proxy
    written in OCaml
        1641 LOC

    simplest program - tiny!
    least configurable / simplest configuration
        rewrites resources to targets

        e.g.,
          deb     http://apt:9999/debian    testing main
          deb     http://apt:9999/security  testing/updates main
          deb-src http://apt:9999/debian    unstable main

          repository     http://remote-host/initial/path

    defaults to port 9999

    runs as user approx (nice)

    cons 
        doesn't support importing

MISC

apt-move    maintain Debian packages as package pool
    move them into heirarchy and generate Packages

* IDEAS

configure files to never expire
configure automatic traffic redirection?
    unless you redirect traffic selectively
        privacy problems
        cache will fill up and kick off older objects


* REMINDER: configuration of squid.conf

http_port 127.0.0.1:3128

adjust refresh patterns with
    refresh_pattern regex min percent max

    options
        override-expire
        ignore-no-cache
        ignore-reload

* RESOURCE: http://www.debian-administration.org/articles/237
    question: how to setup a local debian mirror?

* RESOURCE: http://www.nabble.com/-taprobane--Caching:-apt-cacher-vs-apt-proxy-t935469.html
    bad reviews for apt-proxy
    apt-cacher claimed to be better 

* RESOURCE: https://help.ubuntu.com/community/AptProxy

* RESOURCE: http://apt-proxy.sourceforge.net/

    </t>
<t tx="zaril.20100911091317">SUMMARY
    to do chroot we need suid privileges
    how do we do this correctly?

RESOURCE: http://www.cs.berkeley.edu/~daw/papers/setuid-usenix02.pdf

</t>
<t tx="zaril.20100912044439">* SUMMARY:
    giving up on squid for now - Gentoo package is broken

* GOTCHA:
Sep 12 02:54:38 [(squid)] comm_select_init: epoll_create(): (38) Function not implemented_
Sep 12 02:54:38 [squid] Squid Parent: child process 22865 exited due to signal 6
Sep 12 02:54:41 [squid] Squid Parent: child process 22867 started
Sep 12 02:54:41 [(squid)] comm_select_init: epoll_create(): (38) Function not implemented_
Sep 12 02:54:41 [squid] Squid Parent: child process 22867 exited due to signal 6
Sep 12 02:54:44 [(squid)] comm_select_init: epoll_create(): (38) Function not implemented_
Sep 12 02:54:44 [squid] Squid Parent: child process 22869 started
Sep 12 02:54:44 [squid] Squid Parent: child process 22869 exited due to signal 6

    try using a different configuration?

* RESOURCE: http://gentoo-wiki.com/HOWTO_squid/squidGuard

recommended configuration
    http_port 3128					#squids listening port 3128 is default
    cache_mem 50 MB					#How much memory squid will use for caching 
    visible_hostname my-server			#Name of our server
    cache_dir ufs /var/cache/squid 500 16 256	#Directory where squid stores the cache, 
                            #500 means we use 500MB diskspace for caching
                            #16 and 256 sets how many directorys squid will use, this is default
    offline_mode off				#offline mode is really cool, if activated squid will always use the cache
                            #if the website is not in the cache, squid will fetch it.
                            #if you press reload in your browser, squid will fetch it again.
    maximum_object_size 102400 KB			#if a file is bigger then 102400 KB squid will not cache it.
    reload_into_ims off				#if activated squid will ignore reload requests from browsers, and use
                            #the cache if available
    pipeline_prefetch on				#squid will fetch data parallel
    
    acl my_network src 192.168.0.0/255.255.255.0	#Our network
    acl all src 0.0.0.0/0.0.0.0			#all networks
    
    http_access allow my_network			#allow access for our network
    http_access deny all				#and deny from all others
</t>
<t tx="zaril.20100912052439">debootstrap jaunty jaunty http://apt:9999/ubuntu
</t>
<t tx="zaril.20100912052909">emacs /etc/hosts
    127.0.0.1   apt

ping apt
    
emacs /etc/apt/sources.list
    sed 's/archive.ubuntu.com/apt:9999

apt-get update

import packages from apt-cache?
    no it doesn't work - broken on ubuntu


</t>
<t tx="zaril.20100912055420">maintains a local mirror from various sources

configured by default to create a mirror from apt's cache

caches the control file of packages
    .index/&lt;package&gt;

configuration file
    LOCALDIR        absolute path to your mirror
        e.g., LOCALDIR=/mirrors/ubuntu

    DIST
        e.g., DIST=stable

edit sources.list
    deb file:///mirrors/ubuntu stable main

    
apt-move get
    generates master files (used internally to determine what is available)

apt-move move
    moves files to local mirror location
        LOCALDIR=/mirrors/debian

apt-move packages
    generates the dists pointer (Packages)

GOTCHA:
    apt-move packages aren't authenticated
        http://wiki.debian.org/SecureApt</t>
<t tx="zaril.20100912065617">A2: debootstrap, which uses wget.  This means you can easily install new
    machine using the packages out of your apt-proxy cache.  In boot floppies,
    specify http://APTPROXY:9999/main as your debian mirror (replacing APTPROXY
    with the name or IP address of the machine where apt-proxy is running).

A3: rootstrap, a tool for making root images for user-mode-linux.  Assuming
    that you are running rootstrap on the same machine as apt-proxy and have
    used the default network addresses 192.168.10.x, put this in
    rootstrap.conf:

        mirror=http://192.168.10.1:9999/main

    [Note: during testing, we encountered a strange problem where rootstrap
    thought the architecture was i386-none, so we had to add --arch=i386 to the
    deboostrap call in /usr/lib/rootstrap/modules/debian.]

A4: pbuilder, which also uses debootstrap.  Add this to /etc/pbuilderrc:

        MIRRORSITE=http://APTPROXT:9999/main
        NONUSMIRRORSITE=http://APTPROXT:9999/non-US
</t>
<t tx="zaril.20100916032546">SUMMARY
    location of Packages/Sources
        /var/lib/apt/lists

build dependencies
    *-dev (of libraries)

some build dependencies conflict
    different packages depend on different conflicting versions
    some build dependencies may have `conflicts' headers that limit each other
        e.g., some mtas

Q: how much are build dependencies actually a problem?
    Q: how many build-dependencies? (in all of Debian)
        (download Sources)

Q: is it practical to just install all build dependencies?

Q: how many? how large?

INSIGHT: the lack of build dependencies is a QA feature
    catch missing build-depends

* analysis of build-dependencies in Ubuntu

cat /var/lib/apt/lists/*Sources | grep-dctrl -n -s Build-Depends  ""|sed 's/, /\n/g'|sort -u &gt; build-depends
    7238 different packages with different versions

cat build-depends | perl -pe 's/\s*[,\|]\s*/\n/g' | perl -pe 's/\s*\(.*//; s/^\s*//; s/\[.*//' | sort -u &gt; build-depends-pkgs
    3078 different packages (multiple versions counted)

apt-cache show $(cat build-depends-pkgs) | grep-dctrl -n -s Installed-Size "" | awk '{ sum += $1 } END { print sum }' 
    8GB worth of build-dependencies


IDEA: extract build dependencies for the packages in world

cat /turnkey/repository/*/*/Packages &gt; sphinx
apt-cache showsrc $(cat sphinx | grep-dctrl -n -s Package "" |sort -u) &gt; sphinx-sources
    567 binaries
    546 sources

cat sphinx-sources |grep-dctrl -n -s Build-Depends "" | perl -pe 's/\s*[,\|]\s*/\n/g' |sort -u &gt; sphinx-sources-builddeps 
    742 build depends (with different versions)

cat sphinx-sources-builddeps |  perl -pe 's/\s*\(.*//; s/^\s*//; s/\[.*//' |sort -u &gt; sphinx-sources-builddeps-pkgs
    438 build depends (different versions counted once)

apt-cache show $(cat sphinx-sources-builddeps-pkgs) | grep-dctrl -n -s Installed-Size "" | awk '{ sum += $1 } END { print sum }'
    771MB

</t>
<t tx="zaril.20100918092126">apt-build
debian-builder




</t>
<t tx="zaril.20100923043734">assume get_sources returns:
    /path/to/package: version

get_sources logic
    walk the tree
        any directory that has debian/ in it
            and is not under debian/

Q: how do we get the version?
parse output from dpkg-parsechangelog
    Version: ...
        
        calls /usr/lib/dpkg/parsechangelog/debian
            strips trailing whitespace
            skips newlines
        
                ^(\w[-+0-9a-z.]*) \(([^\(\) \t]+)\)((\s+[-+0-9a-z.]+)+)\;/i
parse the changelog ourselves
</t>
<t tx="zaril.20100923071058"></t>
<t tx="zaril.20100923072629">getpath logic
    try to get the package from the cache, but if we can't:
        search stocks to find source path matching package/version
        deckdebuild the package with the provided build root
        insert any matching built debs into the cache
            if it doesn't already exist?
        insert build log
        return path of package in cache

POOL_TMPDIR     defaults to TMPDIR or /var/tmp

$POOL_TMPDIR/&lt;package&gt;-&lt;version&gt;/
    &lt;package&gt;-&lt;version&gt;
    *.debs


* IDEAS
fix deckdebuild so you can choose where to output debs to
    this way we can save the double copy of the source dir
   
* QUESTIONS:

Q: what do we do if the package fails to build?
A: 
ideas
    output the failed build output + raise exception
---
Q: do we care which stock it came from?
A: not really the source is a full filesystem path (unless we want to report an error)
    the stock just points to it

* TODO:
debsrc.get_name
pool.tmpdir
</t>
<t tx="zaril.20100923143609">TODO: fixed untagged auto-versioning

features
    support branches in Git repositories

subset list logic
    light check out of the branch we want
    detect all Debian directories
    run sourceversion-list on all Debian directories
    
subset getpath logic
    light check out of the branch we want
    seek version we want in temporary checkout
    build package

tests/milestones
    setup a new pool with the following stocks
        use pyproject in various versions
            create a devel version of pyproject
    
    register stable branch of a tagged project
        checked out branch - devel
            should ignore devel versions

        goals
            when we list we should see only the stable version
            get that stable version built

    register a Git repository with two branches
        one branch empty (checked out)
        two debian packages in the other branch

        goals
            when we list we should see only the stable versions
            get that stable versions built

   register a Sumo repository
</t>
<t tx="zaril.20100923230802">SUMMARY
    sbuild mails build logs to maintainer

    builds from dsc

Q: how is sbuild different from pbuilder?
A: doesn't build the package directly from source (pdebuild)
(though generating source package is not a problem)
</t>
<t tx="zaril.20100929193931">SUMMARY
    goal: checkout a specific branch to a separate working directory as efficiently as possible
    insight: we may to modify this script somewhat to work with Sumo arenas
        use sumo-checkout instead of git-checkout
            maybe add the -q option to sumo-checkout

    git-clone cli options
        -s --shared
            clones repository using alternates (no hardlinks)
                very efficient 
    
        -n --no-checkout
            doesn't check out the current branch

   
cli
    cd /path/to/new
    git-clone-checkout path-to-git-repository branch

logic
    check that branch exists on src

    git-clone -n -s src dst
    cd dst
    
    rm -f .git/refs/remotes/origin/HEAD
    mv .git/refs/remotes/origin/* .git/refs/heads

    git-checkout -q &lt;branch&gt;

test
    cd /turnkey/tmp/testgit
    rm -rf test
    git-clone-branch ../pyproject/ master test

</t>
<t tx="zaril.20100930001758">trailing zeros not considered
    -2010 == 0-2010
    
    \d-2010 &gt; 0-2010

    2010 == 2010-0000

    1.01 == 1.1

    1.02 &gt; 1.1  

    2010.1 &gt; 2010-1

    2010:1 &gt; 2010.1

logic
    remove trailing zeros after any non-number

IDEA: translate sort algorithm directly to python?
    just remove trailing zeros?
</t>
<t tx="zaril.20100930010726">TEST
    get pool to give you an older package than is available
        put newer package in subpool
        older package in cache will be returned

IDEAS

cli level solution
    make getpath check for explicit version

    pass unversioned packages to list for versioning
        could return no matching packages
            verify that package exists first
            raise exception if pool-list returns non-zero

        add results to list of packages

        con
            inefficient for small amount of packages?
                it seems we need to do this anyway, so better not replicate this functionality





        
</t>
<t tx="zaril.20100930021923">problem: modification of default array value</t>
<t tx="zaril.20101003042126"></t>
<t tx="zaril.20101003051253">
test: 
    register /turnkey/tmp/verseek/git-single    
        should register checked out branch (e.g., master)

    register /turnkey/tmp/verseek/git-single#devel
        register devel branch


logic
    parse path
    if branch isn't None and path isn't a git repository, raise exception

register logic
    dir, branch = parse stock 
    check that the directory exists
    try to create a git instance at that directory
        if stock is a git repo and branch is none:
            branch = currently checked out branch</t>
<t tx="zaril.20101003051253.1">roadmap
    specific branches unregistered
        require explicit branch specification

implicit branch unregistration
    if branch is None
        list all stocks that have a matching dir linked
            if there is more than one raise an exception
            otherwise, thats the stock we'll delete

    this also work for plain directories



</t>
<t tx="zaril.20101003051313"></t>
<t tx="zaril.20101003054139">test
    pool-register /turnkey/tmp/verseek/git-single#devel
    pool-exists pyproject=1.0

    also want to test binaries

get_sources doesn't work
    

return value
    stock_name, relative_path/package-bin, version

    in a single
        stock_name, '', version

roadmap
    no caching
    caching + naive cache update
        if anything changes, rescan stock
        you always read from the cache, you just make sure its updated first

Q: do we create version caches for regular directories?
    whats the point?
    it would simplify the interface?
    the cached versions could simply be updated every time for regular directories
        would represent a simplified view of the regular directory
            from the pools POV

logic
    update versions for all stocks
    build data structure that represents perceived versions
        stock_name, relative_path/package, versions...

    each stock should know how to 
        updates its own versions
        scan itself for binaries

* IDEAS

the sync phase shouldn't have to happen twice
    once for binaries and once for sources
    checking out stock is an expensive operation (potentially)

Q: what does the interface have to look like to support that?

stock capable of checking out to internal location

sync interface? 
at the stocks level and at the stock level
pass cache object if it finds a binary

* TODO
    fix get_source_versions
        should use verseek to get the versions


Stock interface
    sync    
        gets binaries and updates versions

    checkout
        checks out latest branch

    versions
        (relative/path, versions)
</t>
<t tx="zaril.20101003054139.1"></t>
<t tx="zaril.20101003054139.2">logic
    verseek to desired version
    chdir to &lt;workdir&gt;/relative_path
    deckdebuild
    import built package into cache
    return the path to the built package

</t>
<t tx="zaril.20101003091746">psuedo-logic

HEAD is a special file

checkout the correct branch if git repository

if it is a git repository, only rescan it if there are new updates
if it isn't, scan it every time

scan logic

    scan for debian packages
        debian packages may not include other debian packages or debian binaries
    
    if we find a binary, add it to the cache
    if we find a source, add it to the version


test
    at level of individual Stock
        create package cache object

checkout logic
    if checkout repo doesn't exist create it
        set atlernates to point to original repo

    update/create the desired branch on checkout
        git-update-ref refs/heads/&lt;branch&gt; &lt;commit&gt;
    
    checkout the desired branch    
        git-checkout -q -f &lt;branch&gt;

    create HEAD
</t>
<t tx="zaril.20101005045129">IDEA: 


IDEAS: 

if we already have access to the objects, why not use it to do a more efficient checkout
    e.g., I think checkout is more efficient than deleting the workspace and starting over

don't use clone at all
    specify alternates directly
        by extending Git

    saves us mucking around deleting/moving remote references

create the references automatically also

Q: do I need branches? can I just checkout to HEAD?
A:
    yes you need branches if you are going to checkout past commits
        this way you have something to come back to
---
Q: does git-checkout update everything or just the stuff we need?
A: just the stuff that changed - its very efficient

TODO

* test direct alternates creation
git-init
echo /path/to/orig/.git/objects &gt; .git/objects/info/alternates

git-update-ref refs/heads/devel 0fb016541ae7608a137c9bdf1d047e87e7a6991b
git-checkout -q -f devel






    </t>
<t tx="zaril.20101005065057">GOTCHA: rev-parse tag^0 != rev-parse tag
    the first returns the commit pointed to by the tag, the second returns the tag object
</t>
<t tx="zaril.20101005094458">detect logic
    arena.internals?
    sumo-base
        would work anywhere in the arena (union, internals, etc)

logic
    we need to open the arena in the checkout directory
    we to adjust the directory we scan

</t>
<t tx="zaril.20101005221943">Q: how do we handle subpools?
A:
    we'll need to show the full stock path, not just the name
        package=version stock_path[#branch] relative_path

* IDEAS:
pool-info or pool-list?
    we could show which packages exist in pool-info too
        as headers

but pool-list already has the cli for that
    but it was designed for something else entirely

extend pool-info
    --stocks                show stocks (default)
    --subpools              show sub-poolsl
    --buildroot             show buildroot
    --pkgcache              show package cache
    --source-versions       show source versions</t>
<t tx="zaril.20101006063626">StockSubpool should detect circular dependencies
    if its link is in the recursed_paths

register/unregister

Q: can a new-class object inherit from an old-style object?
</t>
<t tx="zaril.20101006073117">slowing everything down (even pool-info)
magical object with cache

or set it to None on initialisation and init on demand
</t>
<t tx="zaril.20101006120036">--registered (default)
    shows stocks and subpools

--stocks
    only shows stocks
--subpools
    only shows subpools</t>
<t tx="zaril.20101007060755">cli syntax
    pool-info-build source[=version]

will return info on built package
    or no information if it wasn't built
    or an error that the package doesn't exist

GOTCHA: this is tricky because the source isn't automatically associated with the binary

IDEAS:
    store build log in stocks with the same relative paths as source_versions?
        one build log per version per relative_path

        so we would look up a build log
            looking up the correct source_version
            peeking into the build log directory with the same relative path
                for that same version

    allow looking up packages by source name?
        with preference to binary or build?

    lookup build log recursively in all pools?
    
    add buildlogs to info command

        shows which build logs exist
            one way of seeing the source package names

        --source-builds?
        --buildlogs
        --source-build-logs

    if we've already built the package, we can lookup the Source
        by extracting it from the deb's control file

Q: how do I find the Source: field of a package?
    I guess its Source if the Package name isn't the same as the Source

A:
    Source field in the deb's control file if Source != Package
        otherwise no Source field

    E: try changing the Package: to be different from Source: in hello-debhelper
        dpkg -f package.deb
    R: Source </t>
<t tx="zaril.20101007100036">logic
    lookup the binary
    if the binary has a source, lookup the source
    else the source is the binary name

TEST: with hello-debhelper -&gt;  foo
</t>
<t tx="zaril.20101007101419"></t>
<t tx="zaril.20101007101547"></t>
<t tx="zaril.20101007151726"></t>
<t tx="zaril.20101009193905"></t>
<t tx="zaril.20101011023841">SUMMARY:
    version format
        [epoch:]upstream_version[-debian_revision
            
            convention to start deban_revision at 1 each time upstream version increases

    the following don't work
        0turnkey-&lt;revision&gt;
        0-turnkey-&lt;revision&gt;

    only the ubuntu way works
        0turnkey&lt;revision&gt;


    examples
        $ dpkg --compare-versions 1.0-2 gt 1.0-1-turnkey; echo $?
        1
        
        dpkg --compare-versions 1.0-1 gt 1.0-0-turnkey; echo $?
        1

        $ dpkg --compare-versions 1.0-2 gt 1.0-1-; echo $?
        1            



    goal: understand why these tests evaluate the way they do

    package version is broken at the last hyphen
        so upstream can be complex the debian revision is what counts?

    simplest test that demonstrates this behavior
        dpkg --compare-versions 1-1 lt 1-2; echo $? (only a hyphen added)
        0

        dpkg --compare-versions 1-1- lt 1-2; echo $?
        1
            explanation
                1-1 &gt; 1

IDEA: recreate comparison algorithm




        </t>
<t tx="zaril.20101011042720">dpkg --compare-versions 0-2010.10.5-d6cbb928 lt 0-2010.10.10-a9ee521c; echo $?

isolated bug:
    "10" &lt; "5"

compare("10", 5") == -1</t>
<t tx="zaril.20101011052050">IDEAS:
    compare with dpkg --compare-versions based implementation
        272 per/sec with subprocess.call
        227 per/sec with os.system

    GOTCHA: you can't let dpkg --compare-versions do the comparison for you anyhow
        it only returns True or False

    implement in pyrex or C?

    compile regexps?
        up to 4509         

    psyco
        another 40% improvement
            try:
               import psyco; psyco.full()
            except ImportError:
               pass
        
    replace regular expressions
        30% improvement
    
        psyco produces another 30% improvement

    flatten loops
        didn't do much

    make inline
    
    flatten class usage
        17% improvement (6500)
        psyco boosts this implementation 30% (9000)

    normalize is a hog
        176% improvement (18K)
        psyco now boosts an additional 144% (to 44K)

optimization bottom line
    before: 3K/sec
    after: 44K/sec (x14 improvement)

python /usr/lib/python2.4/profile.py -s cumulative ./debversion.py</t>
<t tx="zaril.20101011074222">INSIGHT: we don't need normalize at all!

</t>
<t tx="zaril.20101014093949">. &gt; ~
non-letters should sort earlier than letters
    'a' &lt; '@'
</t>
<t tx="zaril.20101018013654"></t>
<t tx="zaril.20101018013654.1"></t>
<t tx="zaril.20101018013654.2">IDEAS
    PackageCache should precache contents of .pool/pkgcache
        PackageCache.getpath is where most of the time is being spent

    sync as a parameter to the pool?
        
    test in p4 (lots of packages)

Q: why is providing a list so much slower?
A: because the list is verified through pool.exists

</t>
<t tx="zaril.20101018013654.3">logic
    clear everything out of the cache except binary packages/versions that we have sources for
        create a whitelist of package versions, delete from the cache everything not on the whitelist

    clear the SYNC_HEAD of stocks so they get rescanned and we get binaries

    clean all subpools
        optionally?
        if its a safe behavior it should be the default


alternative names
    clean
    refresh
    cleanstale
    refreshcache
    updatecache
    clear
    sync
    gc




</t>
<t tx="zaril.20101018204912">auto-cleaning
    won't work for binary imports
        you'll need to invoke pool-gc to clean up those
</t>
<t tx="zaril.20101019010153">logic
    create cache at init
    append cache on add
    remove from cache on remove
    list cache on list

data structure
    map_filenames
        (name, version) =&gt; filename

    map_versions
        name =&gt; [version1, version2...]

* before

    $ time pool-get out $(pool-list)
    
    real    0m13.066s
    user    0m9.981s
    sys     0m3.052s
    
    $ time pool-get out
    
    real    0m6.752s
    user    0m5.292s
    sys     0m1.340s
    
* after 

    52X speedup

    $ time pool-get repo/ $(pool-list)
    
    real    0m0.255s
    user    0m0.176s
    sys     0m0.044s
    
    44X

    $ time pool-get repo/
    
    real    0m0.156s
    user    0m0.112s
    sys     0m0.008s
    </t>
<t tx="zaril.20101019010223"></t>
<t tx="zaril.20101021175833">test cases
    pool-info-build source
    pool-info-build source=1.2.3

    pool-info-build binary
    pool-info-build binary=1.2.3

Q: how do we get the newest version of a binary?
A:
    we have to list all packages in all subpools, and then select the newest
        (this is done via list)


resolve
    input: unresolve package[s] 
    output: resolved package

rewritten logic
    try to lookup a build log for the package id provided
        this will work if the package's source name is the same as the binary name

    otherwise, try to lookup the source name for the binary package id spec
        if the package spec has a version component 
            lookup that exact name=version in the pool's pkgcache
                extract the source_name from the binary deb

        else:
            extract source name from the newest cached binary package with that name

    afer we have the source_name, try to lookup a build log again (under source_name)



        </t>
<t tx="zaril.20101022061317">if you add a package to the control file:
    pool-list will show versions of the package that never existed

if you remove a package from the control file:
    pool-list won't show package versions that had previously existed

workaround: don't add/remove packages in an existing project - change the name?

so it shows autoversions for packages that never existed
and if you change the packag</t>
<t tx="zaril.20101025111704"></t>
<t tx="zaril.20101101154405">1) have the pool internal directories setuid to the user who initialized it? If root executes the pool, the pool should create a file inside the internals it should be owned by 
2) have the pool temporarily drop privileges to the user who owns the pool (worse)</t>
<t tx="zaril.20101105195018">IDEAS
    perform all operations via subpool
    test stock sources should be part of the suite
    buildroot should also be part of the test suite
        script that performs all of these operations
    script should fail on error
    tee script output and compare with reference?
        minus random shit

    create a tarball of the testsuite contents
    unregister everything before deleting
./

* stock types
    regular directory
    git repository
        non-checked out branch
    git single
    sumo repository
    sub pool

* operations
init

register
unregister
info
    all options

exists package &amp;&amp; exists package=version
    binary that exists
    source version
    source version that doesn't exist
    random non-existant foo package

list 
    should match specific number of versions
list -a
list -n

get
    info-build # should fail
    get non-built package
    info-build # should succeed    
    get cached package
    get binary package
        add binary to regular repository

commit to a package
    exists: check that the new version exists?
    list: check that we can see it?

garblage collection
    unregister branch with sources
    gc
    try to get previously cached binary # should fail


    commit to all stocks and see if the new version is registered

    </t>
<t tx="zaril.20101111222506">IDEAS
    sumo related code needs to be cleaned up?
    detect a Sumo type Stock and create separate class for handling Sumo stocks?</t>
<t tx="zaril.20101112071603">maybe start with plain test and move up?
</t>
</tnodes>
</leo_file>
